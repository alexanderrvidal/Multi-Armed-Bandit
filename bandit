import numpy as np
import matplotlib.pyplot as plt

# Synthetic bandit arms with different success probabilities
class Bandit:
    def __init__(self, arm_probs):
        self.arm_probs = arm_probs  # List of probabilities for each arm
        
    def pull(self, arm):
        # Return 1 for success, 0 for failure based on arm probability
        return 1 if np.random.random() < self.arm_probs[arm] else 0

# Epsilon-Greedy algorithm
class EpsilonGreedy:
    def __init__(self, epsilon, n_arms):
        self.epsilon = epsilon
        self.n_arms = n_arms
        self.counts = np.zeros(n_arms)  # Number of times each arm was pulled
        self.values = np.zeros(n_arms)  # Estimated value of each arm
        
    def select_arm(self):
        if np.random.random() < self.epsilon:
            # Explore: random arm
            return np.random.randint(self.n_arms)
        else:
            # Exploit: best known arm
            return np.argmax(self.values)
    
    def update(self, arm, reward):
        self.counts[arm] += 1
        n = self.counts[arm]
        # Update average using incremental rule: new_avg = old_avg + (reward - old_avg)/n
        self.values[arm] += (reward - self.values[arm]) / n

# Thompson Sampling algorithm (Beta-Bernoulli model)
class ThompsonSampling:
    def __init__(self, n_arms):
        self.n_arms = n_arms
        self.alpha = np.ones(n_arms)  # Success count prior
        self.beta = np.ones(n_arms)   # Failure count prior
        
    def select_arm(self):
        # Sample from Beta distribution for each arm
        samples = [np.random.beta(self.alpha[i], self.beta[i]) for i in range(self.n_arms)]
        return np.argmax(samples)
    
    def update(self, arm, reward):
        # Update alpha (successes) and beta (failures)
        self.alpha[arm] += reward
        self.beta[arm] += (1 - reward)

# Experiment parameters
np.random.seed(42)
arm_probs = [0.2, 0.5, 0.7]  # True success probabilities for each arm
n_arms = len(arm_probs)
n_trials = 1000

# Initialize bandit and strategies
bandit = Bandit(arm_probs)
epsilon_greedy = EpsilonGreedy(epsilon=0.1, n_arms=n_arms)
thompson = ThompsonSampling(n_arms=n_arms)

# Track rewards over time
greedy_rewards = np.zeros(n_trials)
thompson_rewards = np.zeros(n_trials)

# Run trials
for t in range(n_trials):
    # Epsilon-Greedy
    arm = epsilon_greedy.select_arm()
    reward = bandit.pull(arm)
    epsilon_greedy.update(arm, reward)
    greedy_rewards[t] = reward

    # Thompson Sampling
    arm = thompson.select_arm()
    reward = bandit.pull(arm)
    thompson.update(arm, reward)
    thompson_rewards[t] = reward
    print('greedy_rewards', greedy_rewards[t], 'thompson_rewards', thompson_rewards[t])
# Calculate cumulative rewards
cumulative_greedy = np.cumsum(greedy_rewards)
cumulative_thompson = np.cumsum(thompson_rewards)

# Plot results
plt.figure(figsize=(12, 6))
plt.plot(cumulative_greedy, label='ε-Greedy (ε=0.1)')
plt.plot(cumulative_thompson, label='Thompson Sampling')
plt.xlabel('Trials')
plt.ylabel('Cumulative Reward')
plt.title('Multi-Armed Bandit Performance Comparison')
plt.legend()
plt.grid(True)
plt.show()

# Print final arm selection counts
print("Arm probabilities:", arm_probs)
print("ε-Greedy arm selections:", epsilon_greedy.counts)
print("Thompson arm selections:", thompson.alpha[1:] - 1)  # Subtract prior counts